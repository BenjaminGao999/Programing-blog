# 分布式锁的必要保证

1. 加锁和解锁操作必须是原子性的

2. 互斥锁

3. 加锁的client或线程挂了，锁必须被释放掉


4. 保证高可用性,避免单节点问题

5. 多节点下的保证加锁和释放锁的一致性

6. 加锁和解锁的高性能问题


7. 可重入锁

可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。

可重入锁最大的作用是避免死锁。


8. 死锁、活锁、 饥饿
 死锁是相互等待。 活锁相互礼让。 饥饿是永远轮不到执行。

 9. 阻塞和非阻塞




一些实现分布式锁的文章

https://www.hollischuang.com/archives/1716

  
# MySQL分布式锁
 借助事务的特性和回滚机制、超时连接事务失败并回滚，实现MySQL自动释放由下线的客户端添加的锁。
 
 1. 锁粒度
  锁住某个共享资源、 锁住某个操作、 锁住某个方法、 全局锁（所有用户中只能一个获取锁）、 用户锁（用户级别的锁）

 根据业务可以划分多种锁粒度

 2. 可重入锁
在加锁之前，先查询对应的锁是否已经存在了。


3. 实现

 关闭事务， 直接插入，保证数据库存在这个锁

开始事务， 更新这条记录的 lock字段为true

update tb_lock set lock=true  # 阻塞更新

执行业务逻辑


释放锁
 
 update tb_lock set lock=false   # 阻塞更新

事务提交

伪代码： 
   public Object lockAndExec(Object agrs){
        //  关闭事务， 直接插入，保证数据库存在这个锁

        // 开始事务， 更新这条记录的 lock字段为true
        //update tb_lock set lock=true  # 阻塞更新


        //执行业务逻辑
       Object result =  dosometing(agrs);


       //释放锁
        // update tb_lock set lock=false   # 阻塞更新
        //事务提交

       return result;
    }

    private Object dosometing(Object agrs) {
        return null;
    }

可以用装饰者模式， 静态代理、jdk动态代理、或者AOP， 来实现


实现了互斥、阻塞、当client下线，事务自动解锁； 高可用。

但是占用了连接池的连接数。加锁的事务和业务代码的事务用同一个事务的话， 就不用多占用一个连接数了。





# Redis实现分布式锁



http://zhangtielei.com/posts/blog-redlock-reasoning.html
http://www.redis.cn/topics/distlock.html


Redis高可用集群
主从架构
 主节点写， 从节点复制主节点， 当主节点下线由从节点升级为主节点。

Redis 主从架构实现了高可用性， 但是牺牲了一致性  

**Redis 为什么基于故障转移的实现还不够**

为了更好的理解我们想要改进的方面，我们先分析一下当前大多数基于Redis的分布式锁现状和实现方法.

实现Redis分布式锁的最简单的方法就是在Redis中创建一个key，这个key有一个失效时间（TTL)，以保证锁最终会被自动释放掉（这个对应特性2）。当客户端释放资源(解锁）的时候，会删除掉这个key。

从表面上看，似乎效果还不错，但是这里有一个问题：这个架构中存在一个严重的单点失败问题。如果Redis挂了怎么办？你可能会说，可以通过增加一个slave节点解决这个问题。但这通常是行不通的。这样做，我们不能实现资源的独享,因为Redis的主从同步通常是异步的。

在这种场景（主从结构）中存在明显的竞态:

客户端A从master获取到锁

在master将锁同步到slave之前，master宕掉了。

slave节点被晋级为master节点

客户端B取得了同一个资源被客户端A已经获取到的另外一个锁。安全失效！

有时候程序就是这么巧，比如说正好一个节点挂掉的时候，多个客户端同时取到了锁。如果你可以接受这种小概率错误，那用这个基于复制的方案就完全没有问题。


**使用缓存实现分布式锁的缺点**


通过超时时间来控制锁的失效时间并不是十分的靠谱。



# Zookeeper

其实，使用Zookeeper也有可能带来并发问题，只是并不常见而已。考虑这样的情况，由于网络抖动，客户端可ZK集群的session连接断了，那么zk以为客户端挂了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了。就可能产生并发问题。这个问题不常见是因为zk有重试机制，一旦zk集群检测不到客户端的心跳，就会重试，Curator客户端支持多种重试策略。多次重试之后还不行的话才会删除临时节点。（所以，选择一个合适的重试策略也比较重要，要在锁的粒度和并发之间找一个平衡。）

Zookeeper和client的连接和感知基于 session





